textmorph/edit_model/training_run.py
=======================================================
lhs: 100644 | 943edd2afa4644d27ece57ff83fb623cd79fa9d7
rhs: 100644 | 121f1214e2d2772cbb465887beb5a1d5269df0f1
---@@ -18,7 +18,7 @@ from fabric.api import local
 import gtd.io
 from gtd.chrono import verboserate
 from gtd.log import Metadata
-from gtd.utils import random_seed, sample_if_large, bleu, gleu, ribes, chrf, Failure, Config, chunks
+from gtd.utils import random_seed, sample_if_large, bleu, gleu, ribes, chrf, levenshtein_distance, Failure, Config, chunks
 from gtd.ml.training_run import TrainingRunWorkspace, TrainingRuns
 from gtd.ml.torch.training_run import TorchTrainingRun
 from textmorph import data
@@ -52,6 +52,7 @@ class EditTrainingRuns(TrainingRuns):
         gleu = meta.get('gleu_valid', None)
         ribes = meta.get('ribes_valid', None)
         chrf = meta.get('chrf_valid', None)
+        dist = meta.get('dist_valid', None)
         loss = meta.get('loss_valid', None)
         dirty_repo = meta.get('dirty_repo', '?')
 
@@ -59,9 +60,9 @@ class EditTrainingRuns(TrainingRuns):
         config = Config.from_file(workspace.config)
         dataset = config.dataset.path
 
-        return '{name:10} -- steps: {steps:<10}, loss: {loss:.2f}, dset: {dset:15}, bleu: {bleu:.2f}, gleu: {gleu:.2f}, ribes: {ribes:.2f}, chrf: {chrf:.2f} ' \
+        return '{name:10} -- steps: {steps:<10}, loss: {loss:.2f}, dset: {dset:15}, bleu: {bleu:.2f}, gleu: {gleu:.2f}, ribes: {ribes:.2f}, chrf: {chrf:.2f}, dist: {dist:.2f} ' \
                'dirty_repo: {dirty_repo}'.format(
-                name=name, dset=dataset, steps=steps, loss=loss, bleu=bleu, gleu=gleu, ribes=ribes, chrf=chrf, dirty_repo=dirty_repo)
+                name=name, dset=dataset, steps=steps, loss=loss, bleu=bleu, gleu=gleu, ribes=ribes, chrf=chrf, dist=dist, dirty_repo=dirty_repo)
 
     def summarize(self, fmt=None, verbose=False):
         if fmt is None:
@@ -114,13 +115,12 @@ class EditDataSplits(object):
 
             with codecs.open(path, 'r', encoding='utf-8') as f:
                 for line in verboserate(f, desc='Reading data file.', total=total_lines):
+                    # comment out original src, trg
                     # src, trg = line.strip().lower().split('\t')
+
+                    # use new src, trg
                     src = line.strip().lower()
                     trg = src
-                    # print src
-                    # assert(False)
-                    # print src, trg
-                    # assert(False)
                     src_words = src.split(' ')
                     trg_words = trg.split(' ')
                     assert len(src_words) > 0
@@ -133,9 +133,11 @@ class EditDataSplits(object):
                     examples.append(ex)
             return examples
 
+        # # for .tsv files
         # self.train = examples_from_file(join(data_dir, 'train.tsv'))
         # self.valid = examples_from_file(join(data_dir, 'valid.tsv'))
         # self.test = examples_from_file(join(data_dir, 'test.tsv'))
+        # for .txt files
         self.train = examples_from_file(join(data_dir, 'train.txt'))
         self.valid = examples_from_file(join(data_dir, 'valid.txt'))
         self.test = examples_from_file(join(data_dir, 'test.txt'))
@@ -495,7 +497,7 @@ class EditTrainingRun(TorchTrainingRun):
             big_str = 'big_' if big_eval else ''
 
             # compute metrics
-            loss, avg_bleu, avg_gleu, avg_ribes, avg_chrf, edit_traces = cls._compute_metrics(editor, examples, num_eval, noiser,
+            loss, avg_bleu, avg_gleu, avg_ribes, avg_chrf, avg_dist, edit_traces = cls._compute_metrics(editor, examples, num_eval, noiser,
                                                                edit_dropout=config.editor.edit_dropout,
                                                                draw_samples=config.editor.enable_vae)
 
@@ -505,9 +507,10 @@ class EditTrainingRun(TorchTrainingRun):
             log_value('gleu_{}{}'.format(big_str, name), avg_gleu, train_steps)
             log_value('ribes_{}{}'.format(big_str, name), avg_ribes, train_steps)
             log_value('chrf_{}{}'.format(big_str, name), avg_chrf, train_steps)
+            log_value('dist_{}{}'.format(big_str, name), avg_dist, train_steps)
 
             print '=== {}{} ==='.format(big_str, name)
-            print 'loss: {}, bleu: {}, gleu: {}, ribes: {}, chrf: {}'.format(loss, avg_bleu, avg_gleu, avg_ribes, avg_chrf)
+            print 'loss: {}, bleu: {}, gleu: {}, ribes: {}, chrf: {}, dist: {}'.format(loss, avg_bleu, avg_gleu, avg_ribes, avg_chrf, avg_dist)
 
             # print traces for the small evaluation
             if not big_eval:
@@ -540,7 +543,7 @@ class EditTrainingRun(TorchTrainingRun):
 
         # compute BLEU score and log to TensorBoard
         outputs, edit_traces = editor.edit(noised_sample)
-        bleus, gleus, ribeses, chrfs = [], [], [], []
+        bleus, gleus, ribeses, chrfs, dists = [], [], [], [], []
 
         for ex, output in izip(noised_sample, outputs):
             # outputs is a list(over batches)[ list(over beams) [ list(over tokens) [ unicode ] ] ] object.
@@ -548,8 +551,10 @@ class EditTrainingRun(TorchTrainingRun):
             gleus.append(gleu(ex.target_words, output[0]))
             ribeses.append(ribes(ex.target_words, output[0]))
             chrfs.append(chrf(ex.target_words, output[0]))
+            dists.append(levenshtein_distance(ex.target_words, output[0]))
         avg_bleu = np.mean(bleus)
         avg_gleu = np.mean(gleus)
         avg_ribes = np.mean(ribeses)
         avg_chrf = np.mean(chrfs)
-        return loss, avg_bleu, avg_gleu, avg_ribes, avg_chrf, edit_traces
+        avg_dist = np.mean(dists)
+        return loss, avg_bleu, avg_gleu, avg_ribes, avg_chrf, avg_dist, edit_traces

---